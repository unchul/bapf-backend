from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import csv
import requests
from datetime import datetime
from webdriver_manager.chrome import ChromeDriverManager

# í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •
options = webdriver.ChromeOptions()
service = Service(ChromeDriverManager().install())

# ì¹´ì¹´ì˜¤ ì£¼ì†Œ ê²€ìƒ‰ API í‚¤ (í•„ìˆ˜ ì…ë ¥)
KAKAO_API_KEY = '6d9dc3df95f90cbe474e8b518e13f2f2'

# ì„œìš¸ì‹œ êµ¬ ë¦¬ìŠ¤íŠ¸
seoul_gu_list = [
    "ê°•ë‚¨êµ¬", "ê°•ë™êµ¬", "ê°•ë¶êµ¬", "ê°•ì„œêµ¬", "ê´€ì•…êµ¬", "ê´‘ì§„êµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬",
    "ë…¸ì›êµ¬", "ë„ë´‰êµ¬", "ë™ëŒ€ë¬¸êµ¬", "ë™ì‘êµ¬", "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì„œì´ˆêµ¬", "ì„±ë™êµ¬",
    "ì„±ë¶êµ¬", "ì†¡íŒŒêµ¬", "ì–‘ì²œêµ¬", "ì˜ë“±í¬êµ¬", "ìš©ì‚°êµ¬", "ì€í‰êµ¬", "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ì¤‘ë‘êµ¬"
]

# ì¢Œí‘œ ë³€í™˜ í•¨ìˆ˜
def get_lat_lon(address):
    url = "https://dapi.kakao.com/v2/local/search/address.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"query": address}
    response = requests.get(url, headers=headers, params=params)

    if response.status_code == 200:
        result = response.json().get('documents', [])
        if result:
            return result[0]['y'], result[0]['x']
    return None, None

# í¬ë¡¬ ë“œë¼ì´ë²„ ì´ˆê¸°í™”
def init_driver():
    driver = webdriver.Chrome(service=service, options=options)
    driver.get('https://map.kakao.com/')
    time.sleep(2)
    return driver

# ê²€ìƒ‰ì–´ ì…ë ¥ ë° ê²€ìƒ‰ ì‹¤í–‰
def search_keyword(driver, keyword):
    search_area = driver.find_element(By.ID, 'search.keyword.query')
    search_area.clear()
    search_area.send_keys(keyword)
    search_area.send_keys(Keys.ENTER)
    time.sleep(3)
    remove_dimmed_layer(driver)
    click_place_tab(driver)
    click_place_more(driver)

# íŒì—… ì œê±°
def remove_dimmed_layer(driver):
    try:
        dimmed_layer = driver.find_element(By.ID, 'dimmedLayer')
        driver.execute_script("arguments[0].style.display='none';", dimmed_layer)
    except:
        pass

# ì¥ì†Œ íƒ­ í´ë¦­
def click_place_tab(driver):
    WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.XPATH, '//*[@id="info.main.options"]/li[2]/a'))
    ).click()
    time.sleep(2)

# ì¥ì†Œ ë”ë³´ê¸° í´ë¦­
def click_place_more(driver):
    try:
        driver.find_element(By.ID, 'info.search.place.more').click()
        time.sleep(2)
    except:
        pass

# ë©”ë‰´ ìˆ˜ì§‘ í•¨ìˆ˜
def expand_menu_tab_and_collect(driver):
    menu_items = []
    try:
        menu_tab = driver.find_element(By.CSS_SELECTOR, 'a[href="#menuInfo"]')
        menu_tab.click()
        time.sleep(2)

        while True:
            try:
                more_button = driver.find_element(By.CSS_SELECTOR, '.wrap_more a.link_more')
                if more_button.is_displayed():
                    more_button.click()
                    time.sleep(2)
                else:
                    break
            except:
                break

        menu_elements = driver.find_elements(By.CSS_SELECTOR, '.list_goods > li')
        for element in menu_elements:
            name = element.find_element(By.CSS_SELECTOR, '.tit_item').text.strip()
            try:
                price = element.find_element(By.CSS_SELECTOR, '.desc_item').text.strip()
            except:
                price = 'ê°€ê²©ì •ë³´ ì—†ìŒ'
            menu_items.append(f'{name} ({price})')

    except Exception as e:
        print(f"âŒ ë©”ë‰´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return ['ë©”ë‰´ ì—†ìŒ']

    return menu_items if menu_items else ['ë©”ë‰´ ì—†ìŒ']

# ë§¤ì¥ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
def get_store_details(driver, detail_url):
    original_window = driver.current_window_handle
    driver.execute_script("window.open(arguments[0]);", detail_url)
    WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) > 1)
    driver.switch_to.window(driver.window_handles[-1])
    time.sleep(2)
    menu_text = ', '.join(expand_menu_tab_and_collect(driver))
    driver.close()
    driver.switch_to.window(original_window)
    return menu_text

# ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§ (ì„œë¸Œì¹´í…Œê³ ë¦¬ ì¶”ê°€)
def crawl_all_pages(driver):
    all_data = []

    def process_current_page():
        nonlocal all_data
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        stores = soup.select('.placelist > .PlaceItem')

        for store in stores:
            try:
                name = store.select_one('.head_item .tit_name .link_name').text.strip()
                degree = store.select_one('.rating .score .num').text.strip()
                review_count = store.select_one('.review em[data-id="numberofreview"]').text.strip() or '0'
                address = store.select_one('.info_item .addr').text.strip()
                tel = store.select_one('.info_item .phone').text.strip() if store.select_one('.info_item .phone') else 'ì „í™”ë²ˆí˜¸ ì—†ìŒ'
                detail_url = store.select_one('.contact .moreview')['href']
                subcategory = store.select_one('.head_item .subcategory').text.strip()

                menu_text = get_store_details(driver, detail_url)

                print(f"ğŸ“ {name} | ì„œë¸Œì¹´í…Œê³ ë¦¬: {subcategory} | í‰ì : {degree} | ë¦¬ë·° {review_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                all_data.append([name, subcategory, degree, review_count, address, tel, menu_text])

            except Exception as e:
                print(f"âŒ ë§¤ì¥ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    page = 1
    while True:
        process_current_page()
        try:
            if page % 5 == 0:
                next_btn = driver.find_element(By.ID, 'info.search.page.next')
                if "disabled" in next_btn.get_attribute("class"):
                    break
                next_btn.click()
            else:
                driver.find_element(By.ID, f'info.search.page.no{page % 5 + 1}').click()
            page += 1
            time.sleep(2)
        except:
            break
    return all_data

# CSV ì €ì¥ (subcategory ì¶”ê°€)
def save_to_csv(gu_name, data):
    today = datetime.now().strftime("%Y%m%d")
    filename = f'{today}_{gu_name}_ë§›ì§‘_í¬ë¡¤ë§.csv'

    with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['ì´ë¦„', 'ì„œë¸Œì¹´í…Œê³ ë¦¬', 'í‰ì ', 'ë¦¬ë·°ìˆ˜', 'ì£¼ì†Œ', 'ìœ„ë„', 'ê²½ë„', 'ì „í™”ë²ˆí˜¸', 'ë©”ë‰´'])

        for row in data:
            name, subcategory, degree, review_count, address, tel, menu_text = row
            lat, lon = get_lat_lon(address)
            writer.writerow([name, subcategory, degree, review_count, address, lat, lon, tel, menu_text])

    print(f"âœ… {gu_name} ì €ì¥ ì™„ë£Œ ({filename})")

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def crawl_seoul_gu():
    for gu in seoul_gu_list:
        print(f"ğŸ”¹ {gu} í¬ë¡¤ë§ ì‹œì‘!")
        driver = init_driver()
        search_keyword(driver, f'{gu} ë§›ì§‘')
        all_data = crawl_all_pages(driver)
        save_to_csv(gu, all_data)
        driver.quit()
        print(f"âœ… {gu} í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ\n")

if __name__ == '__main__':
    crawl_seoul_gu()
